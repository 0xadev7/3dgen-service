# =========================
# Stage 0: Base (anchor)
# =========================
ARG TORCH_IMAGE="pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel"
FROM ${TORCH_IMAGE} AS base

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_HOME=/usr/local/cuda \
    PIP_NO_CACHE_DIR=1

# Common pins (edit here)
ARG PIN_TRANSFORMERS=">=4.48.2,<5"
ARG PIN_DIFFUSERS="==0.29.2"
ARG PIN_ACCELERATE=">=0.33"
ARG PIN_HUB=">=0.24.6,<1"
ARG PIN_TOKENIZERS=">=0.15"
ARG PIN_SAFETENSORS=">=0.4"
ARG PIN_SENTENCEPIECE=">=0.1.99"
ARG PIN_PROTOBUF=">=3.20,<6"
ARG PIN_XFORMERS="==0.0.28.post3"
ARG PIN_TORCHVISION="==0.18.1"
ARG TRIPOSR_REF="main"
ARG CUDA_ARCHES="8.0;8.6;8.9"

WORKDIR /app


# =========================
# Stage 1: Builder
# =========================
FROM base AS builder

# --- APT with safe TMPDIR (/var/tmp), since NV may be mounted noexec ---
RUN mkdir -p /var/tmp && chmod 1777 /tmp /var/tmp \
 && TMPDIR=/var/tmp apt-get update \
 && TMPDIR=/var/tmp apt-get install -y --no-install-recommends \
      git build-essential libgl1 libglib2.0-0 libxext6 libxrender1 libsm6 libosmesa6 \
      ninja-build cmake python3-dev ca-certificates \
 && rm -rf /var/lib/apt/lists/*

# Caches (created AFTER apt so we can set TMPDIR safely later)
ENV NV=/runpod-volume \
    HF_HOME=/runpod-volume/hf \
    HF_HUB_CACHE=/runpod-volume/hf/hub \
    HF_DATASETS_CACHE=/runpod-volume/hf/datasets \
    TORCH_HOME=/runpod-volume/torch \
    XDG_CACHE_HOME=/runpod-volume/.cache \
    TMPDIR=/runpod-volume/tmp \
    TRANSFORMERS_NO_ADVISORY_WARNINGS=1 \
    HF_HUB_DISABLE_TELEMETRY=1 \
    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
RUN mkdir -p $HF_HOME $HF_HUB_CACHE $HF_DATASETS_CACHE $TORCH_HOME $XDG_CACHE_HOME $TMPDIR

# Wheel cache
RUN mkdir -p /wheelhouse

# Bring app requirements
COPY requirements.txt /tmp/requirements.txt

# Tooling
RUN python -m pip install --upgrade pip setuptools wheel

# 1) Build wheels for app reqs
RUN pip wheel -w /wheelhouse -r /tmp/requirements.txt

# 2) Core pins (ensure exact wheels exist)
RUN pip wheel -w /wheelhouse \
    "transformers${PIN_TRANSFORMERS}" \
    "diffusers${PIN_DIFFUSERS}" \
    "accelerate${PIN_ACCELERATE}" \
    "huggingface-hub${PIN_HUB}" \
    "safetensors${PIN_SAFETENSORS}" \
    "tokenizers${PIN_TOKENIZERS}" \
    "sentencepiece${PIN_SENTENCEPIECE}" \
    "protobuf${PIN_PROTOBUF}" \
    "xformers${PIN_XFORMERS}"

# 2b) TorchVision (from official cu121 index; no deps so we don’t accidentally pull CPU builds)
RUN pip wheel -w /wheelhouse --no-deps \
    --index-url https://download.pytorch.org/whl/cu121 \
    "torchvision${PIN_TORCHVISION}"

# 3) TripoSR: wheel its deps (don’t install yet)
RUN git clone --depth=1 --branch ${TRIPOSR_REF} https://github.com/VAST-AI-Research/TripoSR.git /opt/TripoSR
RUN sed -i '/torchmcubes/d' /opt/TripoSR/requirements.txt \
 && sed -i '/^torch==/d;/^torchvision==/d;/^huggingface-hub==/d;/^transformers==/d' /opt/TripoSR/requirements.txt
RUN pip wheel -w /wheelhouse -r /opt/TripoSR/requirements.txt

# 4) torchmcubes: build wheel with arches
RUN TORCH_CUDA_ARCH_LIST="${CUDA_ARCHES}" MAX_JOBS=4 \
    pip wheel -w /wheelhouse git+https://github.com/tatsy/torchmcubes.git


# =========================
# Stage 2: Runtime
# =========================
FROM ${TORCH_IMAGE} AS runtime

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_HOME=/usr/local/cuda \
    PIP_NO_CACHE_DIR=1

# --- APT with safe TMPDIR (/var/tmp) ---
RUN mkdir -p /var/tmp && chmod 1777 /tmp /var/tmp \
 && TMPDIR=/var/tmp apt-get update \
 && TMPDIR=/var/tmp apt-get install -y --no-install-recommends \
      libgl1 libglib2.0-0 libxext6 libxrender1 libsm6 libosmesa6 ca-certificates \
 && rm -rf /var/lib/apt/lists/*

# Caches (set AFTER apt)
ENV NV=/runpod-volume \
    HF_HOME=/runpod-volume/hf \
    HF_HUB_CACHE=/runpod-volume/hf/hub \
    HF_DATASETS_CACHE=/runpod-volume/hf/datasets \
    TORCH_HOME=/runpod-volume/torch \
    XDG_CACHE_HOME=/runpod-volume/.cache \
    TMPDIR=/runpod-volume/tmp \
    TRANSFORMERS_NO_ADVISORY_WARNINGS=1 \
    HF_HUB_DISABLE_TELEMETRY=1 \
    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

WORKDIR /app
RUN mkdir -p $HF_HOME $HF_HUB_CACHE $HF_DATASETS_CACHE $TORCH_HOME $XDG_CACHE_HOME $TMPDIR

# Wheels + files from builder
COPY --from=builder /wheelhouse /wheelhouse
COPY requirements.txt /tmp/requirements.txt
COPY --from=builder /opt/TripoSR /opt/TripoSR

# Install: wheels first; no git fallbacks in runtime; then re-pin core libs
RUN python -m pip install --upgrade pip setuptools wheel \
 && pip install --no-index --find-links=/wheelhouse -r /tmp/requirements.txt || true \
 && pip install --no-index --find-links=/wheelhouse -r /opt/TripoSR/requirements.txt || true \
 && pip install --no-index --find-links=/wheelhouse torchmcubes* || true \
 && pip install --no-index --find-links=/wheelhouse "torchvision${PIN_TORCHVISION}" \
 # FINAL RE-PINS (force exact versions after anything TripoSR may have pulled)
 && pip install --no-cache-dir --upgrade --upgrade-strategy eager --force-reinstall \
      "transformers${PIN_TRANSFORMERS}" \
      "diffusers${PIN_DIFFUSERS}" \
      "accelerate${PIN_ACCELERATE}" \
      "huggingface-hub${PIN_HUB}" \
      "safetensors${PIN_SAFETENSORS}" \
      "tokenizers${PIN_TOKENIZERS}" \
      "sentencepiece${PIN_SENTENCEPIECE}" \
      "protobuf${PIN_PROTOBUF}" \
      "xformers${PIN_XFORMERS}" \
      "torchvision${PIN_TORCHVISION}"

# ---------- SANITY CHECKS (fail fast) ----------
RUN python - <<'PY'
import importlib, sys
import torch, transformers, diffusers, huggingface_hub
print("torch:", torch.__version__, "cuda:", torch.version.cuda, "CUDA available:", torch.cuda.is_available())
print("diffusers:", diffusers.__version__)
print("transformers:", transformers.__version__)
print("huggingface_hub:", huggingface_hub.__version__)

# torchvision must exist and be compatible
import torchvision, torchvision.ops
print("torchvision:", torchvision.__version__)
import torch as _t
try:
    torchvision.ops.nms(_t.rand(5,4), _t.rand(5), 0.5)
    print("torchvision.ops.nms OK")
except Exception as e:
    print("torchvision.ops.nms FAILED:", e); sys.exit(1)

# 1) UMT5EncoderModel presence (AuraFlow requires this at import time)
assert hasattr(transformers, "UMT5EncoderModel"), "UMT5EncoderModel missing (need transformers >= 4.48)"

# 2) AuraFlow module imports (comment out if PIN_DIFFUSERS == 0.29.2)
importlib.import_module("diffusers.pipelines.aura_flow.pipeline_aura_flow")

# 3) tokenizers / sentencepiece check
from tokenizers import Tokenizer
from transformers import T5Tokenizer
T5Tokenizer.from_pretrained("google/umt5-small")  # tokenizer-only repo; no big weights

# 4) CLIP classes
from transformers import CLIPModel, CLIPProcessor

# 5) xformers present
import xformers, xformers.ops

# 6) diffusers API presence
from diffusers import AutoPipelineForText2Image
print("SANITY CHECKS PASSED")
PY

# ---------- your app ----------
COPY app ./app
COPY scripts ./scripts
ENV TRIPOSR_DIR=/opt/TripoSR

CMD ["python", "-u", "-m", "app.serverless.handler"]


# =========================
# Optional: debug target
# =========================
FROM runtime AS debug
RUN TMPDIR=/var/tmp apt-get update \
 && TMPDIR=/var/tmp apt-get install -y --no-install-recommends git vim less procps \
 && rm -rf /var/lib/apt/lists/*
