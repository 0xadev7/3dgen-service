FROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    TORCH_HOME=/models/torch \
    TRIPOSR_DIR=/opt/TripoSR \
    CUDA_HOME=/usr/local/cuda \
    PIP_NO_CACHE_DIR=1

# Imaging libs + toolchain for CUDA extensions
RUN apt-get update && apt-get install -y --no-install-recommends \
    git build-essential libgl1 libglib2.0-0 libxext6 libxrender1 libsm6 libosmesa6 \
    ninja-build cmake python3-dev ca-certificates \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# --- Your deps ---
COPY requirements.txt ./requirements.txt
RUN sed -i '/^torch==/d;/^torchvision==/d' requirements.txt \
    && python -m pip install --upgrade pip setuptools wheel \
    && pip uninstall -y diffusers transformers huggingface-hub accelerate safetensors || true \
    && pip install --no-cache-dir -r requirements.txt \
    && python - <<'PY'
import diffusers, huggingface_hub
print("diffusers:", diffusers.__version__)
print("huggingface_hub:", huggingface_hub.__version__)
PY

# --- Get TripoSR code ---
ARG TRIPOSR_REF=main
RUN git clone --depth=1 --branch ${TRIPOSR_REF} https://github.com/VAST-AI-Research/TripoSR.git ${TRIPOSR_DIR}

# --- TripoSR deps (skip torchmcubes in file; compile ourselves) ---
RUN sed -i '/torchmcubes/d' ${TRIPOSR_DIR}/requirements.txt \
    && printf "huggingface-hub>=0.24.6,<1\n" > /tmp/constraints.txt \
    && sed -i 's/^transformers==.*/transformers>=4.35,<5/' ${TRIPOSR_DIR}/requirements.txt \
    && sed -i '/^torch==/d;/^torchvision==/d;/^huggingface-hub==/d' ${TRIPOSR_DIR}/requirements.txt \
    && pip install --no-cache-dir -r ${TRIPOSR_DIR}/requirements.txt -c /tmp/constraints.txt --upgrade --upgrade-strategy eager

# Build torchmcubes from source with valid arches
# A100: 8.0 | RTX 30xx: 8.6 | Ada/L40/RTX 6000 Ada: 8.9 | (add ;9.0 for H100)
ARG CUDA_ARCHES="8.0;8.6;8.9"
RUN TORCH_CUDA_ARCH_LIST="${CUDA_ARCHES}" MAX_JOBS=4 \
    pip install --no-cache-dir git+https://github.com/tatsy/torchmcubes.git \
    && python - <<'PY'
import torch, torchmcubes
print("Torch:", torch.__version__, "CUDA:", torch.version.cuda, "CUDA available:", torch.cuda.is_available())
print("torchmcubes:", getattr(torchmcubes, "__version__", "git"))
PY

# --- DO NOT pre-warm HF in the image (saves GBs). Cache at runtime on a volume. ---

# Your app (keep context small with .dockerignore)
COPY app ./app
COPY scripts ./scripts

# Optional: set TRIPOSR_DIR for your app code
ENV TRIPOSR_DIR=/opt/TripoSR

# Runpod
ENV NV=/runpod-volume \
    HF_HOME=/runpod-volume/hf \
    HF_HUB_CACHE=/runpod-volume/hf/hub \
    TRANSFORMERS_CACHE=/runpod-volume/hf/transformers \
    TORCH_HOME=/runpod-volume/torch \
    XDG_CACHE_HOME=/runpod-volume/.cache \
    HF_DATASETS_CACHE=/runpod-volume/hf/datasets \
    TMPDIR=/runpod-volume/tmp
RUN mkdir -p $HF_HOME $HF_HUB_CACHE $TRANSFORMERS_CACHE $TORCH_HOME $XDG_CACHE_HOME $HF_DATASETS_CACHE $TMPDIR

CMD ["python", "-u", "-m", "app.serverless.handler"] 
