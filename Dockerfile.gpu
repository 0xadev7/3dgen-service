FROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    TORCH_HOME=/models/torch \
    TRIPOSR_DIR=/opt/TripoSR \
    CUDA_HOME=/usr/local/cuda \
    PIP_NO_CACHE_DIR=1

# Install system deps FIRST (TMPDIR still defaults to /tmp)
RUN apt-get update && apt-get install -y --no-install-recommends \
    git build-essential libgl1 libglib2.0-0 libxext6 libxrender1 libsm6 libosmesa6 \
    ninja-build cmake python3-dev ca-certificates \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Now set runpod volume-backed caches (including TMPDIR)
ENV NV=/runpod-volume \
    HF_HOME=/runpod-volume/hf \
    TORCH_HOME=/runpod-volume/torch \
    XDG_CACHE_HOME=/runpod-volume/.cache \
    HF_DATASETS_CACHE=/runpod-volume/hf/datasets \
    TMPDIR=/runpod-volume/tmp \
    TRANSFORMERS_NO_ADVISORY_WARNINGS=1 \
    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Make sure those dirs exist for runtime
RUN mkdir -p $HF_HOME $TORCH_HOME $XDG_CACHE_HOME $HF_DATASETS_CACHE $TMPDIR

# ---------- Python deps you control ----------
COPY requirements.txt ./requirements.txt
# We pin/ensure the core stack + sentencepiece (fixes tokenizer error) + xformers (speed)
RUN sed -i '/^torch==/d;/^torchvision==/d' requirements.txt \
    && python -m pip install --upgrade pip setuptools wheel \
    # wipe any pre-pins from earlier layers
    && pip uninstall -y diffusers transformers huggingface-hub accelerate safetensors tokenizers || true \
    # your reqs
    && pip install --no-cache-dir -r requirements.txt \
    # core pins that matter for speed + tokenizer
    && pip install --no-cache-dir \
    "transformers>=4.46,<5" \
    "diffusers>=0.30,<1" \
    "accelerate>=0.33" \
    "safetensors>=0.4" \
    "tokenizers>=0.15" \
    "sentencepiece>=0.1.99" \
    "protobuf>=3.20,<6" \
    "huggingface-hub>=0.24.6,<1" \
    "xformers==0.0.28.post3" || true \
    && python - <<'PY'
import diffusers, huggingface_hub, transformers, torch
print("torch:", torch.__version__, "cuda:", torch.version.cuda)
print("diffusers:", diffusers.__version__)
print("transformers:", transformers.__version__)
print("huggingface_hub:", huggingface_hub.__version__)
PY

# ---------- TripoSR ----------
ARG TRIPOSR_REF=main
RUN git clone --depth=1 --branch ${TRIPOSR_REF} https://github.com/VAST-AI-Research/TripoSR.git ${TRIPOSR_DIR}

# TripoSR deps (skip torchmcubes in file; compile ourselves)
RUN sed -i '/torchmcubes/d' ${TRIPOSR_DIR}/requirements.txt \
    && printf "huggingface-hub>=0.24.6,<1\n" > /tmp/constraints.txt \
    && sed -i 's/^transformers==.*/transformers>=4.35,<5/' ${TRIPOSR_DIR}/requirements.txt \
    && sed -i '/^torch==/d;/^torchvision==/d;/^huggingface-hub==/d' ${TRIPOSR_DIR}/requirements.txt \
    && pip install --no-cache-dir -r ${TRIPOSR_DIR}/requirements.txt -c /tmp/constraints.txt --upgrade --upgrade-strategy eager

# Build torchmcubes from source with valid arches
# A100: 8.0 | RTX 30xx: 8.6 | Ada/L40/RTX 6000 Ada: 8.9 | (add ;9.0 for H100)
ARG CUDA_ARCHES="8.0;8.6;8.9"
RUN TORCH_CUDA_ARCH_LIST="${CUDA_ARCHES}" MAX_JOBS=4 \
    pip install --no-cache-dir git+https://github.com/tatsy/torchmcubes.git \
    && python - <<'PY'
import torch, torchmcubes
print("Torch:", torch.__version__, "CUDA:", torch.version.cuda, "CUDA available:", torch.cuda.is_available())
print("torchmcubes:", getattr(torchmcubes, "__version__", "git"))
PY

# Your app (keep context small with .dockerignore)
COPY app ./app
COPY scripts ./scripts

# Optional: set TRIPOSR_DIR for your app code
ENV TRIPOSR_DIR=/opt/TripoSR

CMD ["python", "-u", "-m", "app.serverless.handler"]
